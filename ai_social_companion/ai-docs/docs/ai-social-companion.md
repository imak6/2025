---
id: ai-social-companion
title: AI Social Companion
sidebar_position: 1
---

# ðŸ§  AI Social Companion Flask Prototype

Welcome to the documentation for the **AI Social Companion**, an open-source Flask web app powered by a local LLM. This app is designed as a foundational prototype for building interactive, personalized assistants that can serve as companionsâ€”especially for users with accessibility needs.

---

## ðŸš€ Overview

This prototype demonstrates:

- A basic chatbot interface served via Flask
- Integration with **local** LLMs like `gemma:2b` using **Ollama**
- A simple user interface to exchange messages with the assistant

---

## ðŸ”§ Features

### âœ… Flask Web Server

- Endpoint: `/chat` accepts POST messages and returns the AI's reply

### âœ… LLM Backend Integration

- Calls local LLM through `http://localhost:11434/api/generate`
- Supports **streamed responses** for faster interactions
- Uses `gemma:2b` (configurable)

### âœ… Simple HTML Chat UI

- Input and display for user and bot messages
- Uses `fetch` API to communicate with the backend in real time

---

## ðŸ—‚ File Structure

ai-companion
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ static
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”œâ”€â”€ templates
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ chatbot.py
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

---

## ðŸ“¦ Setup & Usage

### ðŸ§© 1. Install Ollama and Pull Model

ollama pull gemma:2b

### Run the model locally

ollama run gemma:2b

### Start Flask Server

python app.py

### Access in Browser

Open: [flask_app]

[flask_app]: http://localhost:5000/
